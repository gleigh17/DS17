{"paragraphs":[{"text":"%dep\n\nz.reset()\nz.load(\"joda-time:joda-time:2.9.1\")","dateUpdated":"2017-02-03T19:59:35-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960681_1666159805","id":"20170202-194600_1990002900","dateCreated":"2017-02-03T19:59:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:148","dateFinished":"2017-02-03T19:59:46-0500","dateStarted":"2017-02-03T19:59:35-0500","result":{"code":"SUCCESS","type":"TEXT","msg":"DepInterpreter(%dep) deprecated. Remove dependencies and repositories through GUI interpreter menu instead.\nDepInterpreter(%dep) deprecated. Load dependency through GUI interpreter menu instead.\nres0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@33f8cf7a\n"},"focus":true},{"text":"%spark\n\nimport org.apache.spark.rdd._\nimport scala.collection.JavaConverters._\nimport au.com.bytecode.opencsv.CSVReader","dateUpdated":"2017-02-03T20:00:10-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960683_1665005558","id":"20170202-182955_927526899","dateCreated":"2017-02-03T19:59:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:149","dateFinished":"2017-02-03T20:00:12-0500","dateStarted":"2017-02-03T20:00:10-0500","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.rdd._\n\nimport scala.collection.JavaConverters._\n\nimport au.com.bytecode.opencsv.CSVReader\n"},"focus":true},{"text":"import java.io._\nimport org.joda.time._\nimport org.joda.time.format._\nimport org.joda.time.format.DateTimeFormat\nimport org.joda.time.DateTime\nimport org.joda.time.Days","dateUpdated":"2017-02-03T20:00:20-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960684_1665005558","id":"20170202-183030_972672316","dateCreated":"2017-02-03T19:59:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:150","dateFinished":"2017-02-03T20:00:23-0500","dateStarted":"2017-02-03T20:00:20-0500","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport java.io._\n\nimport org.joda.time._\n\nimport org.joda.time.format._\n\nimport org.joda.time.format.DateTimeFormat\n\nimport org.joda.time.DateTime\n\nimport org.joda.time.Days\n"},"focus":true},{"text":"case class DelayRec(year: String,\n\t\t\t\t\tmonth: String,\n\t\t\t\t\tdayOfMonth: String,\n\t\t\t\t\tdayOfWeek: String,\n\t\t\t\t\tcrsDepTime: String,\n\t\t\t\t\tdepDelay: String,\n\t\t\t\t\torigin: String,\n\t\t\t\t\tdistance: String,\n\t\t\t\t\tcancelled: String) {\n\n\tval holidays = List(\"01/01/2007\", \"01/15/2007\", \"02/19/2007\", \"05/28/2007\", \"06/07/2007\", \"07/04/2007\",\n\t    \"09/03/2007\", \"10/08/2007\" ,\"11/11/2007\", \"11/22/2007\", \"12/25/2007\",\n\t    \"01/01/2008\", \"01/21/2008\", \"02/18/2008\", \"05/22/2008\", \"05/26/2008\", \"07/04/2008\",\n\t    \"09/01/2008\", \"10/13/2008\" ,\"11/11/2008\", \"11/27/2008\", \"12/25/2008\")\n\t\n    def gen_features: (String, Array[Double]) = {\n        val values = Array(\n            depDelay.toDouble,\n            month.toDouble,\n            dayOfMonth.toDouble,  \n            dayOfWeek.toDouble,  \n            get_hour(crsDepTime).toDouble,  \n            distance.toDouble,  \n            days_from_nearest_holiday(year.toInt, month.toInt, dayOfMonth.toInt)  \n        )  \n        new Tuple2(to_date(year.toInt, month.toInt, dayOfMonth.toInt), values)  \n    }\n\n    def get_hour(depTime: String) : String = \"%04d\".format(depTime.toInt).take(2)  \n    def to_date(year: Int, month: Int, day: Int) = \"%04d%02d%02d\".format(year, month, day)  \n  \n    def days_from_nearest_holiday(year:Int, month:Int, day:Int): Int = {  \n        val sampleDate = new org.joda.time.DateTime(year, month, day, 0, 0)\n  \n        holidays.foldLeft(3000) { (r, c) =>  \n        val holiday = org.joda.time.format.DateTimeFormat.forPattern(\"MM/dd/yyyy\").parseDateTime(c)  \n        val distance = Math.abs(org.joda.time.Days.daysBetween(holiday, sampleDate).getDays)  \n        math.min(r, distance)  \n     }  \n    }  \n}  ","dateUpdated":"2017-02-03T20:00:38-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960684_1665005558","id":"20170202-183355_996869266","dateCreated":"2017-02-03T19:59:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:151","dateFinished":"2017-02-03T20:00:29-0500","dateStarted":"2017-02-03T20:00:27-0500","result":{"code":"SUCCESS","type":"TEXT","msg":"\ndefined class DelayRec\n"},"focus":true},{"text":"// function to do a preprocessing step for a given file\ndef prepFlightDelays(infile: String): RDD[DelayRec] = {\n    val data = sc.textFile(infile)\n\n    data.map { line =>\n      val reader = new CSVReader(new StringReader(line))\n      reader.readAll().asScala.toList.map(rec => DelayRec(rec(0),rec(1),rec(2),rec(3),rec(5),rec(15),rec(16),rec(18),rec(21)))\n    }.map(list => list(0))\n    .filter(rec => rec.year != \"Year\")\n    .filter(rec => rec.cancelled == \"0\")\n    .filter(rec => rec.origin == \"ORD\")\n}","dateUpdated":"2017-02-03T20:00:43-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960684_1665005558","id":"20170202-201817_196427194","dateCreated":"2017-02-03T19:59:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:152","dateFinished":"2017-02-03T20:00:45-0500","dateStarted":"2017-02-03T20:00:43-0500","result":{"code":"SUCCESS","type":"TEXT","msg":"\nprepFlightDelays: (infile: String)org.apache.spark.rdd.RDD[DelayRec]\n"},"focus":true},{"text":"val data_2007tmp = prepFlightDelays(\"/Users/datascienceadmin/Downloads/flights_2007.csv.bz2\")\nval data_2007 = data_2007tmp.map(rec => rec.gen_features._2)\nval data_2008 = prepFlightDelays(\"/Users/datascienceadmin/Downloads/flights_2008.csv.bz2\").map(rec => rec.gen_features._2)\n\ndata_2007tmp.toDF().registerTempTable(\"data_2007tmp\")\n\ndata_2007.take(5).map(x => x mkString \",\").foreach(println)","dateUpdated":"2017-02-03T20:01:14-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960684_1665005558","id":"20170202-213830_871416530","dateCreated":"2017-02-03T19:59:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:153","dateFinished":"2017-02-03T20:01:36-0500","dateStarted":"2017-02-03T20:01:14-0500","result":{"code":"SUCCESS","type":"TEXT","msg":"\ndata_2007tmp: org.apache.spark.rdd.RDD[DelayRec] = MapPartitionsRDD[6] at filter at <console>:58\n\ndata_2007: org.apache.spark.rdd.RDD[Array[Double]] = MapPartitionsRDD[7] at map at <console>:52\n\ndata_2008: org.apache.spark.rdd.RDD[Array[Double]] = MapPartitionsRDD[15] at map at <console>:50\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n-8.0,1.0,25.0,4.0,11.0,719.0,10.0\n41.0,1.0,28.0,7.0,15.0,925.0,13.0\n45.0,1.0,29.0,1.0,20.0,316.0,14.0\n-9.0,1.0,17.0,3.0,19.0,719.0,2.0\n180.0,1.0,12.0,5.0,17.0,316.0,3.0\n"},"focus":true},{"text":"%sql\nselect dayofWeek, case when depDelay > 15 then 'delayed' else 'ok' end , count(1)\nfrom data_2007tmp group by dayofweek , case when depDelay > 15 then 'delayed' else 'ok' end","dateUpdated":"2017-02-03T20:03:03-0500","config":{"colWidth":12,"editorMode":"ace/mode/sql","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"dayofWeek","index":0,"aggr":"sum"}],"values":[{"name":"CASE WHEN (CAST(depDelay AS DOUBLE) > CAST(15 AS DOUBLE)) THEN delayed ELSE ok END","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"dayofWeek","index":0,"aggr":"sum"},"yAxis":{"name":"CASE WHEN (CAST(depDelay AS DOUBLE) > CAST(15 AS DOUBLE)) THEN delayed ELSE ok END","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960684_1665005558","id":"20170202-213927_220102195","dateCreated":"2017-02-03T19:59:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:154","dateFinished":"2017-02-03T20:03:59-0500","dateStarted":"2017-02-03T20:03:04-0500","result":{"code":"SUCCESS","type":"TABLE","msg":"dayofWeek\tCASE WHEN (CAST(depDelay AS DOUBLE) > CAST(15 AS DOUBLE)) THEN delayed ELSE ok END\tcount(1)\n1\tdelayed\t16983\n7\tok\t35455\n1\tok\t36815\n6\tdelayed\t10924\n2\tdelayed\t14990\n3\tok\t36975\n4\tdelayed\t16716\n3\tdelayed\t15315\n5\tok\t36823\n2\tok\t37023\n6\tok\t34261\n4\tok\t35680\n5\tdelayed\t16267\n7\tdelayed\t14942\n","comment":"","msgTable":[[{"key":"CASE WHEN (CAST(depDelay AS DOUBLE) > CAST(15 AS DOUBLE)) THEN delayed ELSE ok END","value":"1"},{"key":"CASE WHEN (CAST(depDelay AS DOUBLE) > CAST(15 AS DOUBLE)) THEN delayed ELSE ok END","value":"delayed"},{"key":"CASE WHEN (CAST(depDelay AS DOUBLE) > CAST(15 AS DOUBLE)) THEN delayed ELSE ok END","value":"16983"}],[{"key":"count(1)","value":"7"},{"key":"count(1)","value":"ok"},{"key":"count(1)","value":"35455"}],[{"value":"1"},{"value":"ok"},{"value":"36815"}],[{"value":"6"},{"value":"delayed"},{"value":"10924"}],[{"value":"2"},{"value":"delayed"},{"value":"14990"}],[{"value":"3"},{"value":"ok"},{"value":"36975"}],[{"value":"4"},{"value":"delayed"},{"value":"16716"}],[{"value":"3"},{"value":"delayed"},{"value":"15315"}],[{"value":"5"},{"value":"ok"},{"value":"36823"}],[{"value":"2"},{"value":"ok"},{"value":"37023"}],[{"value":"6"},{"value":"ok"},{"value":"34261"}],[{"value":"4"},{"value":"ok"},{"value":"35680"}],[{"value":"5"},{"value":"delayed"},{"value":"16267"}],[{"value":"7"},{"value":"delayed"},{"value":"14942"}]],"columnNames":[{"name":"dayofWeek","index":0,"aggr":"sum"},{"name":"CASE WHEN (CAST(depDelay AS DOUBLE) > CAST(15 AS DOUBLE)) THEN delayed ELSE ok END","index":1,"aggr":"sum"},{"name":"count(1)","index":2,"aggr":"sum"}],"rows":[["1","delayed","16983"],["7","ok","35455"],["1","ok","36815"],["6","delayed","10924"],["2","delayed","14990"],["3","ok","36975"],["4","delayed","16716"],["3","delayed","15315"],["5","ok","36823"],["2","ok","37023"],["6","ok","34261"],["4","ok","35680"],["5","delayed","16267"],["7","delayed","14942"]]},"focus":true},{"text":"%sql select cast( cast(crsDepTime as int) / 100 as int) as hour,  case when depDelay > 15 then 'delayed' else 'ok' end as delay, count(1) as count from  data_2007tmp group by  cast( cast(crsDepTime as int) / 100 as int),  case when depDelay > 15 then 'delayed' else 'ok' end","dateUpdated":"2017-02-03T20:05:21-0500","config":{"colWidth":12,"editorMode":"ace/mode/sql","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"hour","index":0,"aggr":"sum"}],"values":[{"name":"delay","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"hour","index":0,"aggr":"sum"},"yAxis":{"name":"delay","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960685_1664620809","id":"20170202-215333_1817500793","dateCreated":"2017-02-03T19:59:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:155","dateFinished":"2017-02-03T20:06:21-0500","dateStarted":"2017-02-03T20:05:21-0500","result":{"code":"SUCCESS","type":"TABLE","msg":"hour\tdelay\tcount\n12\tok\t13244\n13\tok\t20922\n20\tdelayed\t10582\n10\tok\t17864\n19\tok\t12774\n15\tok\t14597\n15\tdelayed\t7707\n21\tok\t8048\n8\tok\t20450\n5\tok\t832\n12\tdelayed\t4595\n20\tok\t13791\n10\tdelayed\t5427\n5\tdelayed\t82\n9\tdelayed\t6398\n6\tdelayed\t1966\n6\tok\t15826\n16\tdelayed\t8585\n21\tdelayed\t5427\n13\tdelayed\t7891\n18\tdelayed\t9222\n22\tok\t1637\n16\tok\t14886\n22\tdelayed\t862\n9\tok\t23768\n11\tdelayed\t5314\n14\tok\t13250\n8\tdelayed\t4007\n7\tok\t19420\n11\tok\t15675\n18\tok\t12644\n14\tdelayed\t6345\n17\tok\t13404\n19\tdelayed\t9738\n17\tdelayed\t8696\n7\tdelayed\t3293\n","comment":"","msgTable":[[{"key":"delay","value":"12"},{"key":"delay","value":"ok"},{"key":"delay","value":"13244"}],[{"key":"count","value":"13"},{"key":"count","value":"ok"},{"key":"count","value":"20922"}],[{"value":"20"},{"value":"delayed"},{"value":"10582"}],[{"value":"10"},{"value":"ok"},{"value":"17864"}],[{"value":"19"},{"value":"ok"},{"value":"12774"}],[{"value":"15"},{"value":"ok"},{"value":"14597"}],[{"value":"15"},{"value":"delayed"},{"value":"7707"}],[{"value":"21"},{"value":"ok"},{"value":"8048"}],[{"value":"8"},{"value":"ok"},{"value":"20450"}],[{"value":"5"},{"value":"ok"},{"value":"832"}],[{"value":"12"},{"value":"delayed"},{"value":"4595"}],[{"value":"20"},{"value":"ok"},{"value":"13791"}],[{"value":"10"},{"value":"delayed"},{"value":"5427"}],[{"value":"5"},{"value":"delayed"},{"value":"82"}],[{"value":"9"},{"value":"delayed"},{"value":"6398"}],[{"value":"6"},{"value":"delayed"},{"value":"1966"}],[{"value":"6"},{"value":"ok"},{"value":"15826"}],[{"value":"16"},{"value":"delayed"},{"value":"8585"}],[{"value":"21"},{"value":"delayed"},{"value":"5427"}],[{"value":"13"},{"value":"delayed"},{"value":"7891"}],[{"value":"18"},{"value":"delayed"},{"value":"9222"}],[{"value":"22"},{"value":"ok"},{"value":"1637"}],[{"value":"16"},{"value":"ok"},{"value":"14886"}],[{"value":"22"},{"value":"delayed"},{"value":"862"}],[{"value":"9"},{"value":"ok"},{"value":"23768"}],[{"value":"11"},{"value":"delayed"},{"value":"5314"}],[{"value":"14"},{"value":"ok"},{"value":"13250"}],[{"value":"8"},{"value":"delayed"},{"value":"4007"}],[{"value":"7"},{"value":"ok"},{"value":"19420"}],[{"value":"11"},{"value":"ok"},{"value":"15675"}],[{"value":"18"},{"value":"ok"},{"value":"12644"}],[{"value":"14"},{"value":"delayed"},{"value":"6345"}],[{"value":"17"},{"value":"ok"},{"value":"13404"}],[{"value":"19"},{"value":"delayed"},{"value":"9738"}],[{"value":"17"},{"value":"delayed"},{"value":"8696"}],[{"value":"7"},{"value":"delayed"},{"value":"3293"}]],"columnNames":[{"name":"hour","index":0,"aggr":"sum"},{"name":"delay","index":1,"aggr":"sum"},{"name":"count","index":2,"aggr":"sum"}],"rows":[["12","ok","13244"],["13","ok","20922"],["20","delayed","10582"],["10","ok","17864"],["19","ok","12774"],["15","ok","14597"],["15","delayed","7707"],["21","ok","8048"],["8","ok","20450"],["5","ok","832"],["12","delayed","4595"],["20","ok","13791"],["10","delayed","5427"],["5","delayed","82"],["9","delayed","6398"],["6","delayed","1966"],["6","ok","15826"],["16","delayed","8585"],["21","delayed","5427"],["13","delayed","7891"],["18","delayed","9222"],["22","ok","1637"],["16","ok","14886"],["22","delayed","862"],["9","ok","23768"],["11","delayed","5314"],["14","ok","13250"],["8","delayed","4007"],["7","ok","19420"],["11","ok","15675"],["18","ok","12644"],["14","delayed","6345"],["17","ok","13404"],["19","delayed","9738"],["17","delayed","8696"],["7","delayed","3293"]]},"focus":true},{"text":"%spark\n\nimport org.apache.spark.mllib.regression.LabeledPoint  \nimport org.apache.spark.mllib.linalg.Vectors  \nimport org.apache.spark.mllib.feature.StandardScaler","dateUpdated":"2017-02-03T20:06:38-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960685_1664620809","id":"20170202-215703_2025635005","dateCreated":"2017-02-03T19:59:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:156","dateFinished":"2017-02-03T20:06:34-0500","dateStarted":"2017-02-03T20:06:33-0500","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.mllib.regression.LabeledPoint\n\nimport org.apache.spark.mllib.linalg.Vectors\n\nimport org.apache.spark.mllib.feature.StandardScaler\n"},"focus":true},{"text":"def parseData(vals: Array[Double]): LabeledPoint = {  \n    LabeledPoint(if (vals(0)>=15) 1.0 else 0.0, Vectors.dense(vals.drop(1)))  \n  }  ","dateUpdated":"2017-02-03T20:06:44-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960685_1664620809","id":"20170202-220105_1117977117","dateCreated":"2017-02-03T19:59:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:157","dateFinished":"2017-02-03T20:06:40-0500","dateStarted":"2017-02-03T20:06:39-0500","result":{"code":"SUCCESS","type":"TEXT","msg":"\nparseData: (vals: Array[Double])org.apache.spark.mllib.regression.LabeledPoint\n"},"focus":true},{"text":"// Prepare training set  \nval parsedTrainData = data_2007.map(parseData)  \nparsedTrainData.cache  \nval scaler = new StandardScaler(withMean = true, withStd = true).fit(parsedTrainData.map(x => x.features))  \nval scaledTrainData = parsedTrainData.map(x => LabeledPoint(x.label, scaler.transform(Vectors.dense(x.features.toArray))))  \nscaledTrainData.cache ","dateUpdated":"2017-02-03T20:06:49-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960685_1664620809","id":"20170202-220124_1213118306","dateCreated":"2017-02-03T19:59:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:158","dateFinished":"2017-02-03T20:07:50-0500","dateStarted":"2017-02-03T20:06:49-0500","result":{"code":"SUCCESS","type":"TEXT","msg":"\nparsedTrainData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[49] at map at <console>:60\n\nres11: parsedTrainData.type = MapPartitionsRDD[49] at map at <console>:60\n\nscaler: org.apache.spark.mllib.feature.StandardScalerModel = org.apache.spark.mllib.feature.StandardScalerModel@323dd8b6\n\nscaledTrainData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[52] at map at <console>:63\n\nres12: scaledTrainData.type = MapPartitionsRDD[52] at map at <console>:63\n"},"focus":true},{"text":"// Prepare test/validation set  \nval parsedTestData = data_2008.map(parseData)  \nparsedTestData.cache  \nval scaledTestData = parsedTestData.map(x => LabeledPoint(x.label, scaler.transform(Vectors.dense(x.features.toArray))))  \nscaledTestData.cache","dateUpdated":"2017-02-03T20:07:58-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960685_1664620809","id":"20170202-220422_477738791","dateCreated":"2017-02-03T19:59:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:159","dateFinished":"2017-02-03T20:08:00-0500","dateStarted":"2017-02-03T20:07:58-0500","result":{"code":"SUCCESS","type":"TEXT","msg":"\nparsedTestData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[53] at map at <console>:58\n\nres15: parsedTestData.type = MapPartitionsRDD[53] at map at <console>:58\n\nscaledTestData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[54] at map at <console>:67\n\nres16: scaledTestData.type = MapPartitionsRDD[54] at map at <console>:67\n"},"focus":true},{"text":"scaledTrainData.take(3).map(x => (x.label, x.features)).foreach(println)","dateUpdated":"2017-02-03T20:08:07-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960686_1665775056","id":"20170202-220535_361526916","dateCreated":"2017-02-03T19:59:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:160","dateFinished":"2017-02-03T20:08:08-0500","dateStarted":"2017-02-03T20:08:07-0500","result":{"code":"SUCCESS","type":"TEXT","msg":"(0.0,[-1.6160463330366632,1.0549272994666004,0.03217026353736743,-0.518924417544128,0.03408393342430724,-0.28016830994663705])\n(1.0,[-1.6160463330366632,1.3961052168540338,1.5354307758475594,0.362432098412101,0.4316551188434341,-0.023273887437333846])\n(1.0,[-1.6160463330366632,1.5098311893165115,-1.4710902487728246,1.4641277433573872,-0.7436888225169944,0.062357586732433884])\n"},"focus":true},{"text":"// Function to compute evaluation metrics  \ndef eval_metrics(labelsAndPreds: RDD[(Double, Double)]) : Tuple2[Array[Double], Array[Double]] = {  \n    val tp = labelsAndPreds.filter(r => r._1==1 && r._2==1).count.toDouble  \n    val tn = labelsAndPreds.filter(r => r._1==0 && r._2==0).count.toDouble  \n    val fp = labelsAndPreds.filter(r => r._1==1 && r._2==0).count.toDouble  \n    val fn = labelsAndPreds.filter(r => r._1==0 && r._2==1).count.toDouble  \n  \n    val precision = tp / (tp+fp)  \n    val recall = tp / (tp+fn)  \n    val F_measure = 2*precision*recall / (precision+recall)  \n    val accuracy = (tp+tn) / (tp+tn+fp+fn)  \n    new Tuple2(Array(tp, tn, fp, fn), Array(precision, recall, F_measure, accuracy))  \n    \n}","dateUpdated":"2017-02-03T20:08:20-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960686_1665775056","id":"20170202-220549_1487011295","dateCreated":"2017-02-03T19:59:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:161","dateFinished":"2017-02-03T20:08:21-0500","dateStarted":"2017-02-03T20:08:20-0500","result":{"code":"SUCCESS","type":"TEXT","msg":"\neval_metrics: (labelsAndPreds: org.apache.spark.rdd.RDD[(Double, Double)])(Array[Double], Array[Double])\n"},"focus":true},{"text":"import org.apache.spark.rdd._ \nimport org.apache.spark.rdd.RDD","dateUpdated":"2017-02-03T20:20:49-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960686_1665775056","id":"20170202-220725_1314381042","dateCreated":"2017-02-03T19:59:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:162","dateFinished":"2017-02-03T20:20:51-0500","dateStarted":"2017-02-03T20:20:49-0500","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.rdd._\n\nimport org.apache.spark.rdd.RDD\n"},"focus":true},{"text":"class Metrics(labelsAndPreds: org.apache.spark.rdd.RDD[(Double, Double)]) extends java.io.Serializable {\n\n    private def filterCount(lftBnd:Int,rtBnd:Int):Double = labelsAndPreds\n                                                           .map(x => (x._1.toInt, x._2.toInt))\n                                                           .filter(_ == (lftBnd,rtBnd)).count()\n\n    lazy val tp = filterCount(1,1)  // true positives\n    lazy val tn = filterCount(0,0)  // true negatives\n    lazy val fp = filterCount(0,1)  // false positives\n    lazy val fn = filterCount(1,0)  // false negatives\n\n    lazy val precision = tp / (tp+fp)\n    lazy val recall = tp / (tp+fn)\n    lazy val F1 = 2*precision*recall / (precision+recall)\n    lazy val accuracy = (tp+tn) / (tp+tn+fp+fn)\n}","dateUpdated":"2017-02-03T20:08:38-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960686_1665775056","id":"20170202-220813_767014100","dateCreated":"2017-02-03T19:59:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:163","dateFinished":"2017-02-03T20:08:39-0500","dateStarted":"2017-02-03T20:08:39-0500","result":{"code":"SUCCESS","type":"TEXT","msg":"\ndefined class Metrics\n"},"focus":true},{"text":"%spark \n\nimport org.apache.spark.mllib.classification.LogisticRegressionWithSGD\n\n// Build the Logistic Regression model\nval model_lr = LogisticRegressionWithSGD.train(scaledTrainData, numIterations=100)\n\n// Predict\nval labelsAndPreds_lr = scaledTestData.map { point =>\n    val pred = model_lr.predict(point.features)\n    (pred, point.label)\n}\nval m_lr = eval_metrics(labelsAndPreds_lr)._2\nprintln(\"precision = %.2f, recall = %.2f, F1 = %.2f, accuracy = %.2f\".format(m_lr(0), m_lr(1), m_lr(2), m_lr(3)))","dateUpdated":"2017-02-03T20:08:45-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960686_1665775056","id":"20170202-221349_1240094715","dateCreated":"2017-02-03T19:59:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:164","dateFinished":"2017-02-03T20:09:47-0500","dateStarted":"2017-02-03T20:08:45-0500","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.mllib.classification.LogisticRegressionWithSGD\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nmodel_lr: org.apache.spark.mllib.classification.LogisticRegressionModel = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 6, numClasses = 2, threshold = 0.5\n\nlabelsAndPreds_lr: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[140] at map at <console>:78\n\nm_lr: Array[Double] = Array(0.3735363068960268, 0.6427763108261033, 0.47249298123322336, 0.5915277487847792)\nprecision = 0.37, recall = 0.64, F1 = 0.47, accuracy = 0.59\n"},"focus":true},{"text":"println(model_lr.weights)","dateUpdated":"2017-02-03T20:09:50-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960687_1665390307","id":"20170202-221611_618713045","dateCreated":"2017-02-03T19:59:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:165","dateFinished":"2017-02-03T20:09:50-0500","dateStarted":"2017-02-03T20:09:50-0500","result":{"code":"SUCCESS","type":"TEXT","msg":"[-0.05519239973775392,0.0058773883559942045,-0.03625359858318008,0.3903949271784436,0.04994314670964247,7.940537333813815E-4]\n"},"focus":true},{"text":"%spark\n\nimport org.apache.spark.mllib.tree.DecisionTree\n\n// Build the Decision Tree model\nval numClasses = 2\nval categoricalFeaturesInfo = Map[Int, Int]()\nval impurity = \"gini\"\nval maxDepth = 10\nval maxBins = 100\nval model_dt = DecisionTree.trainClassifier(parsedTrainData, numClasses, categoricalFeaturesInfo, impurity, maxDepth, maxBins)\n\n// Predict\nval labelsAndPreds_dt = parsedTestData.map { point =>\n    val pred = model_dt.predict(point.features)\n    (pred, point.label)\n}\nval m_dt = eval_metrics(labelsAndPreds_dt)._2\nprintln(\"precision = %.2f, recall = %.2f, F1 = %.2f, accuracy = %.2f\".format(m_dt(0), m_dt(1), m_dt(2), m_dt(3)))","dateUpdated":"2017-02-03T20:09:55-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960687_1665390307","id":"20170202-221656_1024116943","dateCreated":"2017-02-03T19:59:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:166","dateFinished":"2017-02-03T20:10:03-0500","dateStarted":"2017-02-03T20:09:55-0500","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.mllib.tree.DecisionTree\n\nnumClasses: Int = 2\n\ncategoricalFeaturesInfo: scala.collection.immutable.Map[Int,Int] = Map()\n\nimpurity: String = gini\n\nmaxDepth: Int = 10\n\nmaxBins: Int = 100\n\nmodel_dt: org.apache.spark.mllib.tree.model.DecisionTreeModel = DecisionTreeModel classifier of depth 10 with 1845 nodes\n\nlabelsAndPreds_dt: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[184] at map at <console>:83\n\nm_dt: Array[Double] = Array(0.40568143388569494, 0.25139360409069955, 0.31042335161991513, 0.6821280529627531)\nprecision = 0.41, recall = 0.25, F1 = 0.31, accuracy = 0.68\n"},"focus":true},{"text":"%spark\n\nimport org.apache.spark.mllib.tree.RandomForest\nimport org.apache.spark.mllib.tree.configuration.Strategy\n\nval treeStrategy = Strategy.defaultStrategy(\"Classification\")\nval numTrees = 20\nval featureSubsetStrategy = \"auto\" // Let the algorithm choose\nval model_rf = RandomForest.trainClassifier(parsedTrainData, treeStrategy, numTrees, featureSubsetStrategy, seed = 123)\n\n","dateUpdated":"2017-02-03T20:10:30-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960687_1665390307","id":"20170202-221805_1767004442","dateCreated":"2017-02-03T19:59:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:167","dateFinished":"2017-02-03T20:10:50-0500","dateStarted":"2017-02-03T20:10:30-0500","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.mllib.tree.RandomForest\n\nimport org.apache.spark.mllib.tree.configuration.Strategy\n\ntreeStrategy: org.apache.spark.mllib.tree.configuration.Strategy = org.apache.spark.mllib.tree.configuration.Strategy@2f8f7ebe\n\nnumTrees: Int = 20\n\nfeatureSubsetStrategy: String = auto\n\n\nmodel_rf: org.apache.spark.mllib.tree.model.RandomForestModel =\nTreeEnsembleModel classifier with 20 trees\n"},"focus":true},{"text":"%spark\n\n// Predict\nval labelsAndPreds_rf = parsedTestData.map { point =>\n    val pred = model_rf.predict(point.features)\n    (point.label, pred)\n}\n\nval m_rf = new Metrics(labelsAndPreds_rf)\nprintln(\"precision = %.2f, recall = %.2f, F1 = %.2f, accuracy = %.2f\"\n        .format(m_rf.precision, m_rf.recall, m_rf.F1, m_rf.accuracy))\n        ","dateUpdated":"2017-02-03T20:10:56-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960687_1665390307","id":"20170202-221905_1558496303","dateCreated":"2017-02-03T19:59:20-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:168","dateFinished":"2017-02-03T20:11:04-0500","dateStarted":"2017-02-03T20:10:56-0500","result":{"code":"SUCCESS","type":"TEXT","msg":"\nlabelsAndPreds_rf: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[228] at map at <console>:82\n\nm_rf: Metrics = Metrics@69986c10\nprecision = 0.49, recall = 0.16, F1 = 0.24, accuracy = 0.71\n"},"focus":true},{"text":"%spark\n\n// import org.apache.spark.rdd._ \n import org.apache.spark.rdd.RDD\nimport org.apache.spark.SparkContext._\nimport scala.collection.JavaConverters._\nimport au.com.bytecode.opencsv.CSVReader\nimport java.io._\n\n// function to do a preprocessing step for a given file\n\ndef preprocess_spark(delay_file: String, weather_file: String): RDD[Array[Double]] = { \n  // Read wether data\n  val delayRecs = prepFlightDelays(delay_file).map{ rec => \n        val features = rec.gen_features\n        (features._1, features._2)\n  }\n\n  // Read weather data into RDDs\n  val station_inx = 0\n  val date_inx = 1\n  val metric_inx = 2\n  val value_inx = 3\n\n  def filterMap(wdata:RDD[Array[String]], metric:String):RDD[(String,Double)] = {\n    wdata.filter(vals => vals(metric_inx) == metric).map(vals => (vals(date_inx), vals(value_inx).toDouble))\n  }\n\n  val wdata = sc.textFile(weather_file).map(line => line.split(\",\"))\n                    .filter(vals => vals(station_inx) == \"USW00094846\")\n  val w_tmin = filterMap(wdata,\"TMIN\")\n  val w_tmax = filterMap(wdata,\"TMAX\")\n  val w_prcp = filterMap(wdata,\"PRCP\")\n  val w_snow = filterMap(wdata,\"SNOW\")\n  val w_awnd = filterMap(wdata,\"AWND\")\n\n  delayRecs.join(w_tmin).map(vals => (vals._1, vals._2._1 ++ Array(vals._2._2)))\n           .join(w_tmax).map(vals => (vals._1, vals._2._1 ++ Array(vals._2._2)))\n           .join(w_prcp).map(vals => (vals._1, vals._2._1 ++ Array(vals._2._2)))\n           .join(w_snow).map(vals => (vals._1, vals._2._1 ++ Array(vals._2._2)))\n           .join(w_awnd).map(vals => vals._2._1 ++ Array(vals._2._2))\n}\n\n\nval data_2007 = preprocess_spark(\"/Users/datascienceadmin/Downloads/flights_2007.csv.bz2\", \"/Users/datascienceadmin/Download/weather_2007.csv.gz\")\nval data_2008 = preprocess_spark(\"/Users/datascienceadmin/Downloads/flights_2008.csv.bz2\", \"/Users/datascienceadmin/Download/weather_2008.csv.gz\")\n\ndata_2007.take(5).map(x => x mkString \",\").foreach(println)","dateUpdated":"2017-02-03T20:54:58-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960687_1665390307","id":"20170202-221958_1809027214","dateCreated":"2017-02-03T19:59:20-0500","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:169","dateFinished":"2017-02-03T20:55:04-0500","dateStarted":"2017-02-03T20:54:58-0500","result":{"code":"ERROR","type":"TEXT","msg":"\nimport org.apache.spark.rdd.RDD\n\nimport org.apache.spark.SparkContext._\n\nimport scala.collection.JavaConverters._\n\nimport au.com.bytecode.opencsv.CSVReader\n\nimport java.io._\n\npreprocess_spark: (delay_file: String, weather_file: String)org.apache.spark.rdd.RDD[Array[Double]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/Users/datascienceadmin/Download/weather_2007.csv.gz\n  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)\n  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)\n  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:304)\n  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n  at org.apache.spark.Partitioner$$anonfun$2.apply(Partitioner.scala:58)\n  at org.apache.spark.Partitioner$$anonfun$2.apply(Partitioner.scala:58)\n  at scala.math.Ordering$$anon$5.compare(Ordering.scala:122)\n  at java.util.TimSort.countRunAndMakeAscending(TimSort.java:355)\n  at java.util.TimSort.sort(TimSort.java:220)\n  at java.util.Arrays.sort(Arrays.java:1438)\n  at scala.collection.SeqLike$class.sorted(SeqLike.scala:648)\n  at scala.collection.AbstractSeq.sorted(Seq.scala:41)\n  at scala.collection.SeqLike$class.sortBy(SeqLike.scala:623)\n  at scala.collection.AbstractSeq.sortBy(Seq.scala:41)\n  at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:58)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$join$2.apply(PairRDDFunctions.scala:652)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$join$2.apply(PairRDDFunctions.scala:652)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n  at org.apache.spark.rdd.PairRDDFunctions.join(PairRDDFunctions.scala:651)\n  at $$$$ce4ac702c30d5973216edd38967eee$$$preprocess_spark(<console>:226)\n  ... 140 elided\n"},"focus":true},{"text":"%spark \n\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.feature.StandardScaler\n\ndef parseData(vals: Array[Double]): LabeledPoint = {\n  LabeledPoint(if (vals(0)>=15) 1.0 else 0.0, Vectors.dense(vals.drop(1)))\n}\n\n// Prepare training set\nval parsedTrainData = data_2007.map(parseData)\nval scaler = new StandardScaler(withMean = true, withStd = true).fit(parsedTrainData.map(x => x.features))\nval scaledTrainData = parsedTrainData.map(x => LabeledPoint(x.label, scaler.transform(Vectors.dense(x.features.toArray))))\nparsedTrainData.cache\nscaledTrainData.cache\n\n// Prepare test/validation set\nval parsedTestData = data_2008.map(parseData)\nval scaledTestData = parsedTestData.map(x => LabeledPoint(x.label, scaler.transform(Vectors.dense(x.features.toArray))))\nparsedTestData.cache\nscaledTestData.cache\n\nscaledTrainData.take(5).map(x => (x.label, x.features)).foreach(println)","dateUpdated":"2017-02-03T20:57:12-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"title":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960687_1665390307","id":"20170202-222109_287663496","dateCreated":"2017-02-03T19:59:20-0500","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:170","dateFinished":"2017-02-03T20:58:19-0500","dateStarted":"2017-02-03T20:57:12-0500","result":{"code":"ERROR","type":"TEXT","msg":"\nimport org.apache.spark.mllib.regression.LabeledPoint\n\nimport org.apache.spark.mllib.linalg.Vectors\n\nimport org.apache.spark.mllib.feature.StandardScaler\n\nparseData: (vals: Array[Double])org.apache.spark.mllib.regression.LabeledPoint\n\nparsedTrainData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[705] at map at <console>:210\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 166.0 failed 1 times, most recent failure: Lost task 1.0 in stage 166.0 (TID 867, localhost): java.lang.ArrayIndexOutOfBoundsException: -1590091655\n\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.getAndMoveToFrontDecode0(CBZip2InputStream.java:1014)\n\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.getAndMoveToFrontDecode(CBZip2InputStream.java:829)\n\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.initBlock(CBZip2InputStream.java:504)\n\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.changeStateToProcessABlock(CBZip2InputStream.java:333)\n\tat org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.read(CBZip2InputStream.java:399)\n\tat org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream.read(BZip2Codec.java:483)\n\tat java.io.InputStream.read(InputStream.java:101)\n\tat org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.fillBuffer(CompressedSplitLineReader.java:130)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)\n\tat org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.readLine(CompressedSplitLineReader.java:159)\n\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:209)\n\tat org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:47)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:255)\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:209)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:147)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1934)\n  at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:983)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n  at org.apache.spark.rdd.RDD.reduce(RDD.scala:965)\n  at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1108)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n  at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1085)\n  at org.apache.spark.mllib.feature.StandardScaler.fit(StandardScaler.scala:57)\n  ... 140 elided\nCaused by: java.lang.ArrayIndexOutOfBoundsException: -1590091655\n  at org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.getAndMoveToFrontDecode0(CBZip2InputStream.java:1014)\n  at org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.getAndMoveToFrontDecode(CBZip2InputStream.java:829)\n  at org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.initBlock(CBZip2InputStream.java:504)\n  at org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.changeStateToProcessABlock(CBZip2InputStream.java:333)\n  at org.apache.hadoop.io.compress.bzip2.CBZip2InputStream.read(CBZip2InputStream.java:399)\n  at org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream.read(BZip2Codec.java:483)\n  at java.io.InputStream.read(InputStream.java:101)\n  at org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.fillBuffer(CompressedSplitLineReader.java:130)\n  at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:216)\n  at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)\n  at org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.readLine(CompressedSplitLineReader.java:159)\n  at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:209)\n  at org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:47)\n  at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:255)\n  at org.apache.spark.rdd.HadoopRDD$$anon$1.getNext(HadoopRDD.scala:209)\n  at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n  at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n  at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)\n  at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)\n  at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)\n  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:147)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n  at org.apache.spark.scheduler.Task.run(Task.scala:85)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n  ... 3 more\n"},"focus":true},{"text":"%spark\n\nimport org.apache.spark.mllib.classification.LogisticRegressionWithSGD\n\n// Build the Logistic Regression model\nval model_lr = LogisticRegressionWithSGD.train(scaledTrainData, numIterations=100)\n\n// Predict\nval labelsAndPreds_lr = scaledTestData.map { point =>\n    val pred = model_lr.predict(point.features)\n    (point.label, pred)\n}\nval m_lr = new Metrics(labelsAndPreds_lr)\nprintln(\"precision = %.2f, recall = %.2f, F1 = %.2f, accuracy = %.2f\"\n        .format(m_lr.precision, m_lr.recall, m_lr.F1, m_lr.accuracy))","dateUpdated":"2017-02-03T20:27:40-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960688_1675778527","id":"20170202-222835_799776088","dateCreated":"2017-02-03T19:59:20-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:171"},{"text":"%spark\n\nprintln(model_lr.weights)","dateUpdated":"2017-02-03T20:27:47-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":false,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960688_1675778527","id":"20170202-222910_870614116","dateCreated":"2017-02-03T19:59:20-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:172"},{"text":"%spark\n\nimport org.apache.spark.mllib.tree.DecisionTree\n\n// Build the Decision Tree model\nval numClasses = 2\nval categoricalFeaturesInfo = Map[Int, Int]()\nval impurity = \"gini\"\nval maxDepth = 10\nval maxBins = 100\nval model_dt = DecisionTree.trainClassifier(parsedTrainData, numClasses, categoricalFeaturesInfo, impurity, maxDepth, maxBins)\n\n// Predict\nval labelsAndPreds_dt = parsedTestData.map { point =>\n    val pred = model_dt.predict(point.features)\n    (point.label, pred)\n}\nval m_dt = new Metrics(labelsAndPreds_dt)\nprintln(\"precision = %.2f, recall = %.2f, F1 = %.2f, accuracy = %.2f\"\n        .format(m_dt.precision, m_dt.recall, m_dt.F1, m_dt.accuracy))","dateUpdated":"2017-02-03T19:59:20-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960688_1675778527","id":"20170202-223139_189650321","dateCreated":"2017-02-03T19:59:20-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:173"},{"text":"%spark\n\nimport org.apache.spark.mllib.tree.RandomForest\nimport org.apache.spark.mllib.tree.configuration.Strategy\n\nval treeStrategy = Strategy.defaultStrategy(\"Classification\")\nval model_rf = RandomForest.trainClassifier(parsedTrainData, \n                                            treeStrategy, \n                                            numTrees = 20, \n                                            featureSubsetStrategy = \"auto\", seed = 125)\n\n// Predict\nval labelsAndPreds_rf = parsedTestData.map { point =>\n    val pred = model_rf.predict(point.features)\n    (point.label, pred)\n}\nval m_rf = new Metrics(labelsAndPreds_rf)\nprintln(\"precision = %.2f, recall = %.2f, F1 = %.2f, accuracy = %.2f\"\n        .format(m_rf.precision, m_rf.recall, m_rf.F1, m_rf.accuracy))","dateUpdated":"2017-02-03T20:26:56-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","lineNumbers":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960688_1675778527","id":"20170202-223213_1080508424","dateCreated":"2017-02-03T19:59:20-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:174"},{"text":"","dateUpdated":"2017-02-03T19:59:20-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1486169960688_1675778527","id":"20170202-223916_2118451567","dateCreated":"2017-02-03T19:59:20-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:175"}],"name":"Lab 2","id":"2C8MMQT7R","angularObjects":{"2C84XJ4S7:shared_process":[],"2C8ZC2YQC:shared_process":[],"2C7YMWV6F:shared_process":[],"2CA4ARHG7:shared_process":[],"2C8Z4VAE3:shared_process":[],"2C9CQ7FST:shared_process":[],"2CA1HQGGG:shared_process":[],"2C8TXXBBR:shared_process":[],"2C9XUTUZ4:shared_process":[],"2CACD5VAN:shared_process":[],"2C9Z7WX8R:shared_process":[],"2C9XPDD71:shared_process":[],"2C9M83YHK:shared_process":[],"2C7TBASDZ:shared_process":[],"2C7VPWMX1:shared_process":[],"2C94FX6VJ:shared_process":[],"2C7Y4C2EH:shared_process":[],"2CANGD8DB:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}